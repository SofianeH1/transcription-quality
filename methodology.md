# ASR quality metrics explained

Hereâ€™s a simple way to think about how weâ€™re judging transcription quality. We look at three things that complement each other:

- WER (Word Error Rate)
- CER (Character Error Rate)
- TFâ€‘IDF cosine similarity
- Latency (ms): total processing time per file (lower is better)
- Realâ€‘Time Factor (RTF = processing_time / audio_duration): lower is better; â‰¤ 1 means realâ€‘time

We also use clear pass/fail thresholds:

```
WER_THRESHOLD = 0.15
CER_THRESHOLD = 0.10
TFIDF_THRESHOLD = 0.75
LATENCY_THRESHOLD_MS=500
RTF_THRESHOLD=0.8
```

If a transcript stays under the WER/CER limits and at or above the TFâ€‘IDF score, itâ€™s in good shape.

---

## 1) WER -- how many words are off

What it tells you:

- How many words are wrong compared to the ground truth, counting insertions, deletions, and substitutions.

Why it matters:

- Itâ€™s the most familiar ASR metric and maps well to human readability. If WER is 10%, roughly 1 in 10 words is wrong.

Quick example:

- Ground truth: â€œThe weather is nice today.â€
- System: â€œThe weather nice today.â€
- Weâ€™re missing â€œisâ€ (a deletion) â†’ 1 error / 5 words = WER = 0.20 (20%).

Threshold: WER â‰¤ 0.15 â†’ at least ~85% wordâ€‘level accuracy.

---

## 2) CER -- when spelling and tiny slips matter

What it tells you:

- The same idea as WER but at the character level. Great for catching small spelling changes and punctuation issues.

Why it matters:

- Sensitive to subtle errors WER can gloss over.
- Helpful for short snippets or languages without spaces.

Quick example:

- Ground truth: â€œcolourâ€
- System: â€œcolorâ€
- One character difference (â€œuâ€) â†’ 1 error / 6 chars = CER â‰ˆ 0.17 (17%). WER could be 0% here, but CER flags the misspelling.

Threshold: CER â‰¤ 0.10 â†’ the text should be cleanly and precisely spelled.

---

## 3) TFâ€‘IDF cosine -- are we saying the same thing?

What it tells you:

- Measures semantic overlap using TFâ€‘IDF and cosine similarity. In other words: do the two texts talk about the same content, even if the wording differs?

Why it matters:

- Captures meaning beyond exact word matches. Useful when thereâ€™s paraphrasing or synonyms (e.g., â€œkidsâ€ vs â€œchildrenâ€).

Quick example:

- Ground truth: â€œThe cat sat on the mat.â€
- System: â€œA cat was sitting on a mat.â€
- Wording changes, but meaning is intact â†’ TFâ€‘IDF cosine â‰ˆ 0.85.

Threshold: TFâ€‘IDF â‰¥ 0.75 â†’ wording can vary, meaning should stay solid.

---

## How to read these together

- WER/CER tell you about surface correctness: wrong or missing words and small textual slips.
- TFâ€‘IDF tells you about meaning: are we conveying the same ideas?

---

## What â€œgoodâ€ looks like at a glance

- WER â‰¤ 0.15
- CER â‰¤ 0.10
- TFâ€‘IDF â‰¥ 0.75
- LATENCY_THRESHOLD_MS â‰¤ 500
- RTF_THRESHOLD â‰¤ 0.8

## Text Transcription Results & Analyse

After running the analyzer script, the following results were obtained:

ğŸ“„ **Transcript 1**

**Accuracy:**
TF-IDF (0.717), WER (0.290), and CER (0.121) all fall below acceptable levels.
This means the transcript contains frequent recognition errors, including missing or substituted words and some spelling issues.

**Performance:**
Latency (450 ms) âœ… and RTF (0.8) âœ… are within limits, showing good speed.

ğŸ§© **Interpretation:**
The system runs efficiently but sacrifices accuracy.
It's fast but unreliable -- likely needs better language modeling or post-processing.

---

ğŸ“„ **Transcript 2**

**Accuracy:**
TF-IDF (0.723), WER (0.226), and CER (0.279) are all well below thresholds, indicating poor transcription quality.
Many small and large recognition errors occur.

**Performance:**
Latency (1200 ms) âŒ and RTF (0.95) âŒ both fail, meaning the system is too slow and not suitable for real-time use.

ğŸ§© **Interpretation:**
This version struggles both in accuracy and responsiveness.
It likely suffers from a heavier model or unstable decoding under test conditions.

---

ğŸ“„ **Transcript 3**

**Accuracy:**
TF-IDF (0.933) âœ…, WER (0.065) âœ…, and CER (0.030) âœ… -- excellent performance across all metrics.
The text is semantically faithful and nearly error-free.

**Performance:**
Latency (400 ms) âœ… and RTF (0.45) âœ… -- fast, responsive, and clearly under all thresholds.

ğŸ§© **Interpretation:**
This configuration delivers both high accuracy and real-time speed.
It's clearly the best result and meets all acceptance criteria.

## References

- https://milvus.io/ai-quick-reference/what-are-common-metrics-for-evaluating-tts-quality
- https://learn.microsoft.com/fr-fr/azure/ai-services/speech-service/how-to-custom-speech-evaluate-data?pivots=ai-foundry-portal
- https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/speech-service/speech-to-text/transparency-note
  https://www.toolify.ai/fr/ai-new-fr/comprendre-et-matriser-tfidf-guide-complet-pour-le-nlp-3514839
